Preciso desenvolver um projeto COMPLETO de AnÃ¡lise de Sentimentos em 
AvaliaÃ§Ãµes de Produtos para disciplina de Processamento Natural de Linguagem, 
combinando SIMPLICIDADE DE EXECUÃ‡ÃƒO com RIGOR CIENTÃFICO.

OBJETIVO: Comparar trÃªs abordagens clÃ¡ssicas + uma moderna (bÃ´nus) em 
reviews de produtos, com validaÃ§Ã£o estatÃ­stica robusta e anÃ¡lises avanÃ§adas 
especÃ­ficas de NLP.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 1: CONFIGURAÃ‡ÃƒO BÃSICA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. COLETA DE DADOS:
   - Produto selecionado: [escolher um produto especÃ­fico]
   - Fonte: Kaggle, Hugging Face, ou web scraping
   - Quantidade MÃNIMA: 3.000 reviews
   - Quantidade IDEAL para validaÃ§Ã£o estatÃ­stica: 5.000+ reviews
   
   - ConversÃ£o de notas para sentimento:
     * Positivo: 4-5 estrelas
     * Negativo: 1-2 estrelas
     * Descartar: 3 estrelas (neutro)
   
   - DivisÃ£o: 70% treino, 15% validaÃ§Ã£o, 15% teste
   - Balanceamento: 50% positivo, 50% negativo
   
   - Metadados adicionais a coletar:
     * review_length (nÃºmero de caracteres)
     * has_emojis (boolean)
     * language_formality (formal/informal - detectar gÃ­rias)
     * contains_typos (simular ou detectar)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 2: MODELOS (4 OBRIGATÃ“RIOS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

2. IMPLEMENTAR QUATRO MODELOS:

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MODELO 1: SVM + Bag of Words (BoW)                                  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ â€¢ VetorizaÃ§Ã£o: TfidfVectorizer (max_features=5000, ngram_range=1-2)â”‚
   â”‚ â€¢ Classificador: SVM (kernel='linear', C=1.0)                      â”‚
   â”‚ â€¢ Baseline clÃ¡ssico e interpretÃ¡vel                                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MODELO 2: SVM + Word Embeddings                                     â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ â€¢ Embeddings: Word2Vec (Google News 300d) ou GloVe                 â”‚
   â”‚ â€¢ AgregaÃ§Ã£o: MÃ©dia ponderada dos vetores (TF-IDF weights)          â”‚
   â”‚ â€¢ Classificador: SVM (kernel='rbf', C=1.0, gamma='scale')          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MODELO 3: BERT Fine-tuned                                           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ â€¢ Modelo: distilbert-base-uncased (mais rÃ¡pido que BERT completo)  â”‚
   â”‚ â€¢ Fine-tuning: 3-5 Ã©pocas, batch_size=16, lr=2e-5                  â”‚
   â”‚ â€¢ Early stopping com validaÃ§Ã£o                                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MODELO 4: LLM com In-Context Learning (BÃ”NUS)                      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ â€¢ Escolher: GPT-4, Claude 3.5 Sonnet, ou Gemini Pro                â”‚
   â”‚ â€¢ Few-shot: 5 exemplos estratÃ©gicos no prompt                      â”‚
   â”‚ â€¢ Zero-shot: Para comparaÃ§Ã£o (se tempo permitir)                   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 3: VALIDAÃ‡ÃƒO ESTATÃSTICA (RIGOR CIENTÃFICO)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3. MÃšLTIPLAS EXECUÃ‡Ã•ES COM DIFERENTES SEEDS:

   a) NÃºmero de simulaÃ§Ãµes:
      - MÃNIMO: 10 simulaÃ§Ãµes por modelo (aceitÃ¡vel)
      - IDEAL: 30 simulaÃ§Ãµes por modelo (robusto)
      - Variar: seed para split dos dados + seed de inicializaÃ§Ã£o (BERT)
   
   b) Para cada simulaÃ§Ã£o extrair:
      âœ“ Accuracy
      âœ“ Precision (macro e por classe)
      âœ“ Recall (macro e por classe)
      âœ“ F1-Score (macro e weighted)
      âœ“ Tempo de treinamento (se aplicÃ¡vel)
      âœ“ Tempo de inferÃªncia (mÃ©dia por review)
   
   c) Armazenar:
      - CSV com mÃ©tricas de TODAS as simulaÃ§Ãµes
      - Formato: [modelo, simulacao_id, accuracy, precision, recall, f1, ...]

4. TESTES ESTATÃSTICOS (Î±=0.05, 95% confianÃ§a):

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TESTE 1: Wilcoxon Signed-Rank Test (pareado)                       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ â€¢ Objetivo: Comparar pares de modelos (ex: BERT vs SVM+BoW)        â”‚
   â”‚ â€¢ HipÃ³tese: H0 = nÃ£o hÃ¡ diferenÃ§a significativa                    â”‚
   â”‚ â€¢ Executar: Para cada par de modelos, em cada mÃ©trica              â”‚
   â”‚ â€¢ Reportar: p-valor, significÃ¢ncia (p<0.05?), vencedor             â”‚
   â”‚                                                                      â”‚
   â”‚ Exemplo de comparaÃ§Ãµes:                                             â”‚
   â”‚ - BERT vs SVM+BoW                                                   â”‚
   â”‚ - BERT vs SVM+Embeddings                                            â”‚
   â”‚ - BERT vs LLM                                                       â”‚
   â”‚ - SVM+Embeddings vs SVM+BoW                                         â”‚
   â”‚ - LLM vs todos (se tempo permitir)                                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TESTE 2: Kruskal-Wallis H-test (nÃ£o-paramÃ©trico, mÃºltiplos grupos)â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ â€¢ Objetivo: Testar se HÃ diferenÃ§a entre os 4 modelos              â”‚
   â”‚ â€¢ HipÃ³tese: H0 = todos os modelos tÃªm mesma mediana                â”‚
   â”‚ â€¢ Executar: Uma vez para cada mÃ©trica (Accuracy, F1, etc.)         â”‚
   â”‚ â€¢ Se p<0.05: HÃ¡ diferenÃ§a significativa â†’ prosseguir com Wilcoxon  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TESTE 3: Teste de Normalidade (Shapiro-Wilk)                       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ â€¢ Objetivo: Verificar se distribuiÃ§Ãµes sÃ£o normais                 â”‚
   â”‚ â€¢ Se normais: Poderia usar t-test (mais poderoso)                  â”‚
   â”‚ â€¢ Se nÃ£o-normais: Wilcoxon Ã© mais apropriado (esperado em ML)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. VISUALIZAÃ‡Ã•ES ESTATÃSTICAS:

   a) Boxplots:
      - Um boxplot POR MÃ‰TRICA (Accuracy, Precision, Recall, F1)
      - Comparando os 4 modelos
      - Mostrar: mediana, quartis, outliers
   
   b) GrÃ¡ficos de linha:
      - EvoluÃ§Ã£o de Accuracy ao longo das simulaÃ§Ãµes
      - EvoluÃ§Ã£o de F1-Score ao longo das simulaÃ§Ãµes
      - Identificar estabilidade vs variabilidade
   
   c) Tabela de significÃ¢ncia:
      - Matriz de p-valores (modelo A vs modelo B)
      - CÃ³digo de cores: verde (p<0.05), vermelho (pâ‰¥0.05)
   
   d) GrÃ¡fico de barras com intervalo de confianÃ§a:
      - MÃ©dia Â± desvio padrÃ£o para cada modelo
      - Ou: MÃ©dia com intervalo de confianÃ§a 95%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 4: ANÃLISES ADICIONAIS ESPECÃFICAS DE NLP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

6. ANÃLISE 1 - EXEMPLOS DE ACERTOS E ERROS:

   Para CADA modelo, identificar e documentar:
   
   a) Casos de ACERTO por todos os modelos:
      - 5-10 exemplos de reviews fÃ¡ceis
      - CaracterÃ­sticas comuns (claros, diretos, vocabulÃ¡rio simples)
   
   b) Casos onde APENAS UM modelo acerta:
      - BERT acerta, SVMs erram â†’ captura contexto complexo
      - SVM+Embeddings acerta, BoW erra â†’ semÃ¢ntica ajuda
      - LLM acerta, todos erram â†’ generalizaÃ§Ã£o superior
   
   c) Casos de ERRO por todos os modelos:
      - Reviews com sarcasmo: "Ã“timo! Quebrou no primeiro dia ğŸ™„"
      - Reviews com ironia: "Adorei esperar 3 meses pela entrega"
      - Reviews ambÃ­guos: "Ã‰ bom... mas esperava mais"
   
   d) AnÃ¡lise de Falsos Positivos e Falsos Negativos:
      - Quais tipos de erro cada modelo comete mais?
      - PadrÃµes linguÃ­sticos que confundem cada modelo

   FORMATO DE APRESENTAÃ‡ÃƒO:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Review: "Produto excelente mas entrega pÃ©ssima"                     â”‚
   â”‚ Label verdadeiro: NEGATIVO                                          â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
   â”‚ SVM+BoW:        POSITIVO âœ— (focou em "excelente")                  â”‚
   â”‚ SVM+Embeddings: NEGATIVO âœ“ (capturou "pÃ©ssima" com peso)           â”‚
   â”‚ BERT:           NEGATIVO âœ“ (entendeu contexto "mas")               â”‚
   â”‚ LLM:            NEGATIVO âœ“ (raciocÃ­nio: entrega > produto)         â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
   â”‚ INSIGHT: "mas" Ã© crucial - BoW nÃ£o captura, outros sim             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

7. ANÃLISE 2 - COMPRIMENTO DE TEXTO vs ACURÃCIA:

   a) Binning de comprimento:
      - Curto: 0-50 caracteres
      - MÃ©dio: 51-200 caracteres
      - Longo: 201-500 caracteres
      - Muito longo: 500+ caracteres
   
   b) Para cada bin e cada modelo, calcular:
      - Accuracy no bin
      - F1-Score no bin
      - NÃºmero de amostras no bin
   
   c) VisualizaÃ§Ã£o:
      - GrÃ¡fico de linha: Eixo X = comprimento, Eixo Y = Accuracy
      - Uma linha por modelo
   
   d) AnÃ¡lise esperada:
      - BoW: pode degradar em textos muito longos (esparsidade)
      - Embeddings: mais robusto a comprimento
      - BERT: limite de 512 tokens - truncamento afeta?
      - LLM: contexto grande, deve ser robusto
   
   e) Teste estatÃ­stico:
      - CorrelaÃ§Ã£o de Pearson/Spearman: comprimento Ã— accuracy
      - Por modelo: hÃ¡ correlaÃ§Ã£o significativa?

8. ANÃLISE 3 - ROBUSTEZ A ERROS DE DIGITAÃ‡ÃƒO/GRAMÃTICA:

   a) Criar dataset perturbado:
      - Pegar 200-500 reviews do conjunto de teste
      - Aplicar perturbaÃ§Ãµes controladas:
        * Trocar 5% das letras aleatoriamente (typos)
        * Remover acentos ("Ã³timo" â†’ "otimo")
        * Duplicar letras ("bom" â†’ "boom")
        * Inverter letras adjacentes ("produto" â†’ "porduto")
   
   b) Avaliar cada modelo:
      - Accuracy no dataset limpo
      - Accuracy no dataset perturbado
      - Queda de performance (Î” Accuracy)
   
   c) VisualizaÃ§Ã£o:
      - GrÃ¡fico de barras: [Limpo | Perturbado] por modelo
      - Calcular % de degradaÃ§Ã£o
   
   d) AnÃ¡lise esperada:
      - BoW: muito sensÃ­vel (palavras fora do vocabulÃ¡rio)
      - Embeddings: mais robusto (palavras similares tÃªm vetores prÃ³ximos)
      - BERT: robusto (subword tokenization - WordPiece)
      - LLM: muito robusto (treinado em textos ruidosos da internet)

9. ANÃLISE 4 - TEXTOS COM EMOJIS:

   a) Separar subconjunto:
      - Reviews COM emojis vs SEM emojis
      - Garantir balanceamento de sentimento em ambos
   
   b) Avaliar cada modelo:
      - Accuracy em reviews SEM emojis
      - Accuracy em reviews COM emojis
      - DiferenÃ§a estatÃ­stica (teste t ou Wilcoxon)
   
   c) AnÃ¡lise de emojis informativos:
      - Positivos: ğŸ˜Š ğŸ‘ â¤ï¸ â­ ğŸ‰
      - Negativos: ğŸ˜¡ ğŸ‘ ğŸ’” ğŸ˜ âš ï¸
      - Neutros: ğŸ¤” ğŸ˜
   
   d) Teste adicional:
      - Remover emojis dos reviews e reclassificar
      - Emojis sÃ£o cruciais ou apenas decorativos?
   
   e) VisualizaÃ§Ã£o:
      - GrÃ¡fico de barras: Accuracy [Com Emojis | Sem Emojis] por modelo
   
   f) AnÃ¡lise esperada:
      - BoW: ignora emojis (trata como tokens desconhecidos)
      - BERT: pode capturar (se fine-tuned com emojis)
      - LLM: forte com emojis (treinamento em redes sociais)

10. ANÃLISE 5 - SARCASMO E IRONIA:

    a) Anotar manualmente subset de sarcasmo:
       - Identificar 50-100 reviews sarcÃ¡sticos/irÃ´nicos no teste
       - Exemplos:
         * "Adorei! Durou impressionantes 2 dias ğŸ‘"
         * "Excelente qualidade... se vocÃª gosta de plÃ¡stico barato"
         * "Recomendo se vocÃª quer jogar dinheiro fora"
    
    b) Avaliar cada modelo:
       - Accuracy no subset sarcÃ¡stico
       - Accuracy no subset NÃƒO-sarcÃ¡stico
       - Comparar com performance geral
    
    c) AnÃ¡lise de features de sarcasmo:
       - PresenÃ§a de "!" mÃºltiplos
       - Palavras extremas ("adorei", "excelente") + sentimento negativo
       - Emojis irÃ´nicos (ğŸ‘ ğŸ‰ usado negativamente)
    
    d) VisualizaÃ§Ã£o:
       - Tabela: Accuracy [Geral | SarcÃ¡stico] por modelo
       - % de queda em reviews sarcÃ¡sticos
    
    e) AnÃ¡lise esperada:
       - BoW: pÃ©ssimo (sÃ³ vÃª palavras positivas, ignora contexto)
       - Embeddings: ligeiramente melhor
       - BERT: melhor (captura contexto, mas ainda desafiador)
       - LLM: melhor performance (raciocÃ­nio de alto nÃ­vel)

11. ANÃLISE 6 - SENSIBILIDADE A IDIOMA/DIALETO/FORMALIDADE:

    a) Categorizar reviews por formalidade:
       - Formal: "O produto apresenta excelente qualidade"
       - Informal: "Produto top demais, curti muito"
       - GÃ­rias: "Produto massa, show de bola, recomendo"
    
    b) Detectar automaticamente:
       - Usar heurÃ­sticas simples:
         * GÃ­rias brasileiras: "top", "massa", "show", "da hora"
         * AbreviaÃ§Ãµes: "vc", "tbm", "mt", "blz"
         * ALL CAPS: "ADOREI", "PÃ‰SSIMO"
    
    c) Avaliar cada modelo:
       - Accuracy em cada categoria de formalidade
       - Teste estatÃ­stico: diferenÃ§a significativa entre categorias?
    
    d) AnÃ¡lise adicional - tratamento de caso:
       - Testar em reviews em UPPERCASE
       - Testar em reviews em lowercase
       - Testar em MiXeD CaSe
    
    e) VisualizaÃ§Ã£o:
       - Heatmap: Linhas=modelos, Colunas=categorias, Valores=Accuracy
    
    f) AnÃ¡lise esperada:
       - BoW: sensÃ­vel se treinou lowercase (gÃ­rias fora vocabulÃ¡rio)
       - BERT: robusto (case-insensitive por padrÃ£o)
       - LLM: muito robusto (viu diversidade linguÃ­stica enorme)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 5: ESTRUTURA DE IMPLEMENTAÃ‡ÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

12. ORGANIZAÃ‡ÃƒO DO CÃ“DIGO:

    projeto_sentiment_nlp/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ raw/                          # Dados originais
    â”‚   â”œâ”€â”€ processed/                    # Dados limpos
    â”‚   â”œâ”€â”€ perturbed/                    # Dataset com typos
    â”‚   â”œâ”€â”€ emoji_analysis/               # Subsets com/sem emojis
    â”‚   â”œâ”€â”€ sarcasm_subset/               # Reviews sarcÃ¡sticos anotados
    â”‚   â””â”€â”€ formality_categories/         # Formal/Informal/GÃ­rias
    â”‚
    â”œâ”€â”€ notebooks/
    â”‚   â”œâ”€â”€ 01_data_collection_eda.ipynb
    â”‚   â”œâ”€â”€ 02_data_preparation.ipynb
    â”‚   â”œâ”€â”€ 03_model_svm_bow.ipynb
    â”‚   â”œâ”€â”€ 04_model_svm_embeddings.ipynb
    â”‚   â”œâ”€â”€ 05_model_bert.ipynb
    â”‚   â”œâ”€â”€ 06_model_llm.ipynb
    â”‚   â”œâ”€â”€ 07_statistical_validation.ipynb
    â”‚   â”œâ”€â”€ 08_error_analysis.ipynb
    â”‚   â”œâ”€â”€ 09_length_analysis.ipynb
    â”‚   â”œâ”€â”€ 10_robustness_typos.ipynb
    â”‚   â”œâ”€â”€ 11_emoji_analysis.ipynb
    â”‚   â”œâ”€â”€ 12_sarcasm_analysis.ipynb
    â”‚   â””â”€â”€ 13_formality_analysis.ipynb
    â”‚
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ data_preprocessing.py
    â”‚   â”œâ”€â”€ data_perturbation.py          # Adicionar typos, etc.
    â”‚   â”œâ”€â”€ model_svm_bow.py
    â”‚   â”œâ”€â”€ model_svm_embeddings.py
    â”‚   â”œâ”€â”€ model_bert.py
    â”‚   â”œâ”€â”€ model_llm.py
    â”‚   â”œâ”€â”€ evaluation_metrics.py
    â”‚   â”œâ”€â”€ statistical_tests.py          # Wilcoxon, Kruskal-Wallis
    â”‚   â”œâ”€â”€ error_analysis.py
    â”‚   â”œâ”€â”€ advanced_analysis.py          # Comprimento, emojis, etc.
    â”‚   â””â”€â”€ utils.py
    â”‚
    â”œâ”€â”€ results/
    â”‚   â”œâ”€â”€ simulations/                  # 10-30 simulaÃ§Ãµes por modelo
    â”‚   â”‚   â”œâ”€â”€ svm_bow_simulations.csv
    â”‚   â”‚   â”œâ”€â”€ svm_emb_simulations.csv
    â”‚   â”‚   â”œâ”€â”€ bert_simulations.csv
    â”‚   â”‚   â””â”€â”€ llm_simulations.csv
    â”‚   â”œâ”€â”€ statistical_tests/
    â”‚   â”‚   â”œâ”€â”€ wilcoxon_results.json
    â”‚   â”‚   â”œâ”€â”€ kruskal_wallis_results.json
    â”‚   â”‚   â””â”€â”€ statistical_report.txt
    â”‚   â”œâ”€â”€ error_analysis/
    â”‚   â”‚   â”œâ”€â”€ examples_correct_all.txt
    â”‚   â”‚   â”œâ”€â”€ examples_bert_only.txt
    â”‚   â”‚   â”œâ”€â”€ examples_incorrect_all.txt
    â”‚   â”‚   â””â”€â”€ confusion_matrices/
    â”‚   â”œâ”€â”€ advanced_analysis/
    â”‚   â”‚   â”œâ”€â”€ length_vs_accuracy.csv
    â”‚   â”‚   â”œâ”€â”€ typos_robustness.csv
    â”‚   â”‚   â”œâ”€â”€ emoji_analysis.csv
    â”‚   â”‚   â”œâ”€â”€ sarcasm_performance.csv
    â”‚   â”‚   â””â”€â”€ formality_analysis.csv
    â”‚   â””â”€â”€ plots/
    â”‚       â”œâ”€â”€ boxplots/
    â”‚       â”œâ”€â”€ line_plots/
    â”‚       â”œâ”€â”€ statistical/
    â”‚       â””â”€â”€ advanced_analysis/
    â”‚
    â”œâ”€â”€ presentation/
    â”‚   â”œâ”€â”€ slides.pptx
    â”‚   â”œâ”€â”€ video_script.md
    â”‚   â””â”€â”€ supplementary_material.pdf
    â”‚
    â”œâ”€â”€ config.py
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md

13. BIBLIOTECAS NECESSÃRIAS:

    # BÃ¡sicas
    pandas>=1.5.0
    numpy>=1.23.0
    matplotlib>=3.6.0
    seaborn>=0.12.0
    
    # NLP ClÃ¡ssico
    scikit-learn>=1.2.0
    nltk>=3.8
    gensim>=4.3.0
    
    # BERT
    transformers>=4.35.0
    torch>=2.0.0
    datasets>=2.14.0
    accelerate>=0.24.0
    
    # LLM APIs (escolher 1)
    openai>=1.3.0
    anthropic>=0.7.0
    google-generativeai>=0.3.0
    
    # EstatÃ­stica
    scipy>=1.10.0
    statsmodels>=0.14.0
    
    # PerturbaÃ§Ã£o de texto
    nlpaug>=1.1.11
    
    # DetecÃ§Ã£o de emoji
    emoji>=2.8.0
    
    # Utilidades
    tqdm>=4.66.0
    joblib>=1.3.0

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 6: APRESENTAÃ‡ÃƒO (15 MINUTOS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

14. ESTRUTURA DOS SLIDES (18-20 slides):

    SLIDE 1:  TÃ­tulo + Objetivo + MotivaÃ§Ã£o
    SLIDE 2:  Dataset (produto, quantidade, balanceamento)
    SLIDE 3:  Metodologia - VisÃ£o Geral (4 modelos + validaÃ§Ã£o)
    
    SLIDE 4:  Modelo 1 - SVM + BoW
    SLIDE 5:  Modelo 2 - SVM + Embeddings
    SLIDE 6:  Modelo 3 - BERT Fine-tuned
    SLIDE 7:  Modelo 4 - LLM In-Context (bÃ´nus)
    
    SLIDE 8:  Resultados Principais - Tabela Comparativa
    SLIDE 9:  ValidaÃ§Ã£o EstatÃ­stica - Wilcoxon + p-valores
    SLIDE 10: Boxplots - DistribuiÃ§Ã£o das MÃ©tricas
    SLIDE 11: GrÃ¡ficos de Linha - Estabilidade
    
    SLIDE 12: AnÃ¡lise de Erros - Exemplos Qualitativos
    SLIDE 13: Comprimento de Texto vs AcurÃ¡cia
    SLIDE 14: Robustez a Typos - Queda de Performance
    SLIDE 15: Emojis e Sarcasmo - Desafios Especiais
    SLIDE 16: Sensibilidade a Formalidade/Dialeto
    
    SLIDE 17: Trade-offs - Performance vs Complexidade vs Custo
    SLIDE 18: ConclusÃµes + RecomendaÃ§Ãµes PrÃ¡ticas
    SLIDE 19: ContribuiÃ§Ãµes + Trabalhos Futuros
    SLIDE 20: Agradecimentos + Q&A

15. ROTEIRO DO VÃDEO (15 minutos):

    00:00-01:00  IntroduÃ§Ã£o + MotivaÃ§Ã£o + Dataset
    01:00-04:00  4 Modelos (45 seg cada)
    04:00-06:00  Resultados + ValidaÃ§Ã£o EstatÃ­stica
    06:00-08:00  AnÃ¡lise de Erros Qualitativos
    08:00-11:00  AnÃ¡lises AvanÃ§adas (comprimento, typos, emojis, sarcasmo)
    11:00-13:00  Trade-offs + RecomendaÃ§Ãµes
    13:00-14:30  ConclusÃµes + ContribuiÃ§Ãµes
    14:30-15:00  Perguntas ou DemonstraÃ§Ã£o ao Vivo (opcional)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 7: ENTREGÃVEIS FINAIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

16. CHECKLIST DE ENTREGÃVEIS:

    CÃ“DIGO E DADOS:
    âœ“ CÃ³digo completo (notebooks ou scripts Python)
    âœ“ Dataset original + processado (CSV)
    âœ“ Datasets perturbados (typos, sem emojis, etc.)
    âœ“ requirements.txt
    âœ“ README com instruÃ§Ãµes completas
    
    RESULTADOS:
    âœ“ CSVs com mÃ©tricas de TODAS as simulaÃ§Ãµes
    âœ“ RelatÃ³rio estatÃ­stico (Wilcoxon, Kruskal-Wallis)
    âœ“ Tabelas de anÃ¡lises avanÃ§adas (comprimento, typos, etc.)
    
    VISUALIZAÃ‡Ã•ES:
    âœ“ Boxplots (Accuracy, Precision, Recall, F1)
    âœ“ GrÃ¡ficos de linha (evoluÃ§Ã£o por simulaÃ§Ã£o)
    âœ“ Matrizes de confusÃ£o (agregadas)
    âœ“ GrÃ¡ficos de anÃ¡lises avanÃ§adas (comprimento, typos, emojis, etc.)
    âœ“ Tabela de p-valores (significÃ¢ncia estatÃ­stica)
    
    ANÃLISES:
    âœ“ Documento com exemplos de acertos/erros (20-30 exemplos anotados)
    âœ“ RelatÃ³rio de anÃ¡lise de comprimento
    âœ“ RelatÃ³rio de robustez a typos
    âœ“ RelatÃ³rio de anÃ¡lise de emojis
    âœ“ RelatÃ³rio de anÃ¡lise de sarcasmo
    âœ“ RelatÃ³rio de anÃ¡lise de formalidade
    
    APRESENTAÃ‡ÃƒO:
    âœ“ Slides (PDF + PPTX)
    âœ“ VÃ­deo (mÃ¡ximo 15 minutos)
    âœ“ Script/roteiro do vÃ­deo
    âœ“ Material suplementar (se necessÃ¡rio)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 8: CRONOGRAMA AJUSTADO (2-3 SEMANAS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

17. PLANO DE EXECUÃ‡ÃƒO:

    SEMANA 1 - IMPLEMENTAÃ‡ÃƒO BÃSICA:
    Dia 1-2:  Coleta de dados + EDA (4h)
    Dia 3:    Preparar datasets perturbados (2h)
    Dia 4:    SVM + BoW (2h)
    Dia 5:    SVM + Embeddings (3h)
    Dia 6-7:  BERT fine-tuning (4h)
    
    SEMANA 2 - EXPERIMENTOS E VALIDAÃ‡ÃƒO:
    Dia 8:    LLM in-context (2h)
    Dia 9-10: Executar 10-30 simulaÃ§Ãµes de cada modelo (6h)
    Dia 11:   Testes estatÃ­sticos (Wilcoxon, Kruskal-Wallis) (2h)
    Dia 12:   AnÃ¡lise de erros qualitativos (3h)
    Dia 13:   AnÃ¡lise de comprimento + typos (3h)
    Dia 14:   AnÃ¡lise de emojis + sarcasmo + formalidade (4h)
    
    SEMANA 3 - APRESENTAÃ‡ÃƒO:
    Dia 15-16: Gerar todas as visualizaÃ§Ãµes (4h)
    Dia 17-18: Preparar slides (4h)
    Dia 19:    Escrever roteiro do vÃ­deo (2h)
    Dia 20:    Gravar e editar vÃ­deo (3h)
    Dia 21:    RevisÃ£o final + ajustes (2h)
    
    TOTAL: ~50-60 horas de trabalho

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTE 9: DIFERENCIAL DESTE PROJETO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

18. O QUE TORNA ESTE PROJETO EXCEPCIONAL:

    RIGOR CIENTÃFICO:
    âœ“ ValidaÃ§Ã£o estatÃ­stica com Wilcoxon (Î±=0.05)
    âœ“ 10-30 simulaÃ§Ãµes (robustez)
    âœ“ MÃºltiplas mÃ©tricas (nÃ£o sÃ³ accuracy)
    âœ“ VisualizaÃ§Ãµes profissionais (boxplots, linhas, heatmaps)
    
    ANÃLISES AVANÃ‡ADAS DE NLP:
    âœ“ Comprimento de texto vs performance
    âœ“ Robustez a typos (perturbaÃ§Ã£o controlada)
    âœ“ AnÃ¡lise de emojis (informatividade)
    âœ“ DetecÃ§Ã£o de sarcasmo/ironia (desafio conhecido)
    âœ“ Sensibilidade a formalidade/dialeto
    
    ANÃLISE QUALITATIVA:
    âœ“ 20-30 exemplos anotados de acertos/erros
    âœ“ IdentificaÃ§Ã£o de padrÃµes de erro
    âœ“ Insights linguÃ­sticos especÃ­ficos
    
    ABORDAGEM MODERNA:
    âœ“ 3 modelos clÃ¡ssicos + 1 estado-da-arte (LLM)
    âœ“ In-context learning (tendÃªncia atual)
    âœ“ ComparaÃ§Ã£o justa (mesmos dados, mesmas mÃ©tricas)
    
    APRESENTAÃ‡ÃƒO IMPACTANTE:
    âœ“ Slides profissionais com dados reais
    âœ“ VÃ­deo bem estruturado (15 min)
    âœ“ AnÃ¡lises prÃ¡ticas e acionÃ¡veis
    âœ“ Material suplementar completo

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Por favor, desenvolva este projeto COMPLETO com:
1. CÃ³digo modular e bem documentado
2. ValidaÃ§Ã£o estatÃ­stica RIGOROSA (Wilcoxon, mÃºltiplas simulaÃ§Ãµes)
3. AnÃ¡lises avanÃ§adas ESPECÃFICAS de NLP (comprimento, typos, emojis, 
   sarcasmo, formalidade)
4. VisualizaÃ§Ãµes profissionais para apresentaÃ§Ã£o
5. DocumentaÃ§Ã£o detalhada de todos os experimentos

Este projeto combina SIMPLICIDADE DE EXECUÃ‡ÃƒO (modelos estabelecidos) com
RIGOR CIENTÃFICO (validaÃ§Ã£o estatÃ­stica + anÃ¡lises profundas), adequado
para disciplina de mestrado ou publicaÃ§Ã£o em workshop de NLP.